マケデコ「Optiverコンペ Kaggle上位解法勉強会」議事録

**開催日時**: 2024年5月23日 19:30〜 **主催**: マケデコ (Market API Developer Community) **司会**: 北山氏

1. 概要

Kaggleで開催されたコンペティション「Optiver - Trading at the Close」の上位解法および参加者による振り返りを共有する勉強会。Nasdaqのクロージング・クロス（大引け）直前の10分間における株価変動を予測する課題に対し、データの基礎解析、銀メダル獲得解法、および上位解法の比較分析が発表された。

\--------------------------------------------------------------------------------

2. 発表内容

(1) データの基礎解析 ＋ 1st Solution 解法共有

**発表者**: 西本氏（nishimoto）

• **コンペ概要とデータ解析**:

  ◦ **課題**: 米国株式市場の終了直前10分間のボラティリティと価格変動を予測する。評価指標はMAE。

  ◦ **データ**: 200銘柄 × 481日分。需給（Bid/Ask）、価格、出来高など17列の基本データが提供された。

  ◦ **特徴**: ターゲット（目的変数）の分布は裾が広く、極端な値が含まれる。また、インバランス（需給の偏り）系の特徴量がターゲットと高い相関を示した。

• **1位解法（HYD氏）の分析**:

  ◦ **モデル構成**: **CatBoost (0.5) + GRU (0.3) + Transformer (0.2)** のアンサンブル。GBDT（決定木）とNN（ニューラルネット）を組み合わせている。

  ◦ **勝因（Magic Features）**: 特定の時間帯（0-300秒、300-480秒、480秒以降）でグループ化し、その中での「初期値との比率」や「移動平均との比率」をとる特徴量が有効であった。

  ◦ **オンライン学習**: 評価期間中に追加されるデータを使ってモデルを再学習（12日ごと）させたことがスコア向上に大きく寄与した。

(2) Optiver参戦記 & 銀メダル解法

**発表者**: tonic氏（89位 / 銀メダル）

• **解法アプローチ**:

  ◦ **モデル**: **LightGBM (0.61) + Transformer (0.39)** のアンサンブル。

  ◦ **特徴量**: 462個の特徴量を作成。各ペアの乖離、インバランス、日内のローリング集約、データセット全体でのグループエンコーディングなどを採用。

  ◦ **Transformerの活用**: 銘柄間の相関（Cross-sectional）を捉えるためにTransformer Encoderを使用。GBDTとは異なる予測特性を持ち、アンサンブルでの寄与度が大きかった（重み約4割）。

• **反省点・上位との差分**:

  ◦ **オンライン学習の欠如**: 計算リソースと時間の制約から再学習を実装できず、ドメインシフト（学習データとテストデータの乖離）に対応しきれなかった点が最大の敗因と分析。

  ◦ **時系列ベクトルの扱い**: GRUやCNNなど、時系列方向の情報をうまく扱うモデルの検討が不足していた。

  ◦ **失敗した試行**: ターゲットを符号と絶対値に分けて予測する2-stage予測や、ボラティリティ予測による補正は効果がなかった。

(3) 上位解法モデル比較

**発表者**: richwomanbtc氏（1836位）

• **上位解法の傾向分析**:

  ◦ 上位陣は共通して「Treeモデル（GBDT）と時系列NNのアンサンブル」「オンライン学習」「時間枠でグループ化した特徴量」「ポストプロセッシング（予測値の合計を0にする等の補正）」を採用していた。

  ◦ 6位はNN単独、9位はXGBoost単独など、バリエーションも存在した。

• **再現実験と考察**:

  ◦ 7位解法をベースに、同一の特徴量を用いて複数モデル（LightGBM, CatBoost, CNN, GRU, LSTM, Transformer）の性能を比較検証。

  ◦ **結果**: 単体モデルではLightGBMが最も精度が高かった。NNモデル群（CNN, LSTM等）も健闘したが、GBDT系の方が扱いやすく精度が出やすい傾向が見られた。

  ◦ **特徴量の重要性**: 「時間帯ごとのグループ化」や「中央値からの乖離」といった、市場構造を反映した特徴量が有効であった。

\--------------------------------------------------------------------------------

3. 質疑応答・主な議論

• **オンライン学習への気づき**: データIDごとの相関係数の推移を見ると、古いデータほど相関が弱くなっている傾向があったため、そこから直近データを重視する（オンライン学習）必要性に気づけた可能性がある。

• **Transformerの欠損値処理**: ファープライス（Far Price）などが前半欠損する場合、定数（1など）で埋める処理が行われていた。

• **モデルの選択**: 最終的にはLightGBMなどのGBDTが強力だが、予測特性の異なるNN（Transformerなど）をアンサンブルに混ぜることでスコアが安定・向上する点が確認された。

4. 所感・まとめ

• 本コンペティションは「シェイク（順位変動）」が少なく、適切なデータ分析とエンジニアリング（特にオンライン学習と特徴量作成）を行った参加者が報われるロバストな設計であった。

• 上位に入るには、GBDTだけでなくNNを含めた多様なモデルのアンサンブルと、計算コストのかかるオンライン学習を実装しきるエンジニアリング力が求められた。